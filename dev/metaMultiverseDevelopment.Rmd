---
title: "Development Workflow for metamultiverse"
author: "Constantin Yves Plessen"
version: "1.0.0"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: show
    number_sections: true
    theme: united
    highlight: zenburn
    df_print: paged
editor_options: 
  chunk_output_type: inline
params:
  colorblind_friendly: TRUE
  ylim_lower: -0.5
  ylim_upper: 2.0
license: "MIT"
github-repo: "cyplessen/metaMultiverse"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
library(ggplot2)
library(plotly)
library(dplyr)
library(glue)
```

# Quick test run

```{r}
devtools::document()
devtools::load_all()
multiverse <- data_digDep %>%
  check_data_multiverse() %>%
  define_factors(
    Population = "wf_3|U",
    Technology = "wf_2|N",
    Risk_of_Bias = list(
      "wf_7",
      decision = "U",
      groups = list( 
        "low_risk_only" = "low risk",
        "exclude_high_risk" = c("low risk", "some concerns"),
        "all_studies" = c("low risk", "some concerns", "high risk")
      )
    )
  ) %>%
  create_multiverse_specifications(ma_methods = "reml") %>% 
  run_multiverse_analysis()

#viz
plot_spec_curve(multiverse,
         interactive = F)

plot_voe(multiverse)
```


#2.0 Worflow

```{r}
devtools::load_all()
# Complete Working Pipeline for Custom Factor Groups
# ================================================

# 1. Prepare data - Make it easier with helper function
create_user_friendly_digDep <- function(data_digDep) {
  
  # Create a copy and rename the wf_* columns to meaningful names
  user_data <- data_digDep
  
  # Rename wf_* columns to what researchers would actually call them
  names(user_data)[names(user_data) == "wf_1"] <- "technology_type"      # website, mobile, etc.
  names(user_data)[names(user_data) == "wf_2"] <- "guidance_level"       # guided, minimal support, etc.
  names(user_data)[names(user_data) == "wf_3"] <- "population"           # adult, etc.
  names(user_data)[names(user_data) == "wf_4"] <- "therapy_approach"     # cbt-based, not-cbt-based, etc.
  names(user_data)[names(user_data) == "wf_5"] <- "control_type"         # waitlist, other control
  names(user_data)[names(user_data) == "wf_6"] <- "diagnostic_status"    # clinical cutoff, etc.
  names(user_data)[names(user_data) == "wf_7"] <- "risk_of_bias"         # some concerns, high risk, etc.
  names(user_data)[names(user_data) == "wf_8"] <- "time_point"           # post-treatment, follow-up
  
  return(user_data)
}

# Start with the data
depression_data <- create_user_friendly_digDep(data_digDep)
data_validated <- check_data_multiverse(depression_data)

# 2. Define which factors with custom groups
setup <- define_factors(
  data_validated,
  Risk_of_Bias = list("risk_of_bias", 
                      decision = "N", 
                      groups = list(
                        "low_risk_only" = "low risk",
                        "exclude_high_risk" = c("low risk", "some concerns"), 
                        "all_studies" = c("low risk", "some concerns", "high risk")
                      )
  ),
  Technology = list("technology_type", 
                    decision = "U",
                    type = "binary"),
  Population = list("population", 
                    decision = "E",
                    type = "binary")
)

# 3. Create specifications with custom groups
custom_specs <- create_principled_multiverse_specifications(
  data = setup$data,  # Use setup$data which has the wf_* columns
  wf_vars = setup$factors$wf_internal,  # Use the internal wf_* names
  ma_methods = c("reml", "fe"),
  dependencies = c("aggregate"),
  decision_map = setup$decision_map,
  factor_groups = setup$factor_groups   # Pass the custom groups
)

# 4. Run analysis
results <- run_multiverse_analysis(
  data = setup$data,  # Use setup$data which has the wf_* columns
  specifications = custom_specs$specifications,
  factor_groups = setup$factor_groups,  # Pass custom groups for filtering
  verbose = TRUE,
  progress = TRUE
)
```



# add heterogenetiy

```{r}
devtools::load_all()
res_fe <- metafor::rma(data = setup$data,
                       yi = yi, vi = vi, method = "FE")
res_fe$I2
res_fe$tau2

res_rml <- metafor::rma(data = setup$data,
                        yi = yi, vi = vi, method = "REML",
                        control = list(stepadj = 0.5, maxiter = 2000))

res_rml$tau2
res_rml$I2

res_pm <- metafor::rma(data = setup$data,
                       yi = yi,
                       vi = vi,
                       method = "PM",
                       test   = "z")
res_pm$tau2
res_pm$I2
res_hk <- meta::metagen(data = setup$data,
                        TE = yi,
                        seTE    = sqrt(vi),
                        method.random.ci = "HK",
                        method.tau       = "SJ")
res_hk$tau2
res_hk$I2

res_ba <- bayesmeta::bayesmeta(y = setup$data$yi,
                               sigma = sqrt(setup$data$vi))
s <- res_ba$summary
res_ba

# extract tau2 from posterior median of sigma (tau)
tau_sd <- s["median", "tau"]
tau2 <- tau_sd^2

# approximate I2 = 100 * tau2 / (tau2 + mean(vi))
i2   <- 100 * tau2 / (tau2 + mean(setup$data$vi, na.rm = TRUE))

mod <- metafor::rma.mv(data = setup$data, 
                       yi = yi, V = vi,
                       random = ~ 1 | study/es_id, 
                       method = "REML",
                       sparse = TRUE)

tau2 <- mod$sigma2[1]
W <- diag(1/mod$vi)
X <- model.matrix(mod)
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
I2 <- 100 * sum(mod$sigma2) / (sum(mod$sigma2) + (mod$k-mod$p)/sum(diag(P)))

```
```{r}
library(dplyr)
library(tibble)

# --- assume you already ran these and have:
# res_fe, res_rml, res_pm, res_hk, res_ba, mod (three‐level)

# 1) extract Bayesian tau2 & I2
s        <- res_ba$summary
tau2_ba  <- (s["median", "tau"])^2
i2_ba    <- 100 * tau2_ba / (tau2_ba + mean(setup$data$vi, na.rm = TRUE))

# 2) extract three‐level tau2 & I2
tau2_ml <- mod$sigma2[1]
W       <- diag(1 / setup$data$vi)
X       <- model.matrix(mod)
P       <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
i2_ml   <- 100 * sum(mod$sigma2) / ( sum(mod$sigma2) + (mod$k - mod$p) / sum(diag(P)) )

# 3) build the table
results_tbl <- tribble(
  ~Method,       ~b,                  ~ci.lb,            ~ci.ub,           ~tau2,          ~I2,
  "FE",          res_fe$b,            res_fe$ci.lb,      res_fe$ci.ub,     res_fe$tau2,    res_fe$I2,
  "REML",        res_rml$b,           res_rml$ci.lb,     res_rml$ci.ub,    res_rml$tau2,   res_rml$I2,
  "Paule–Mandel",res_pm$b,            res_pm$ci.lb,      res_pm$ci.ub,     res_pm$tau2,    res_pm$I2,
  "HK/SJ",       res_hk$TE.random,    res_hk$lower.random,res_hk$upper.random,res_hk$tau2, res_hk$I2,
  "Bayesian",    s["median","mu"],    s["95% lower","mu"],s["95% upper","mu"],tau2_ba,      i2_ba,
  "3‐Level",     mod$b,               mod$ci.lb,         mod$ci.ub,        tau2_ml,        i2_ml
)

print(results_tbl)
```





```{r}
# ================================================
# Alternative: Simplified pipeline using run_multiverse wrapper
# ================================================

cat("\n=== ALTERNATIVE: Using simplified run_multiverse wrapper ===\n")

# This would be the simplified approach (once run_multiverse is implemented)
# results_simple <- run_multiverse(setup, 
#                                  methods = c("reml", "fe"),
#                                  dependencies = c("aggregate"),
#                                  verbose = TRUE)

# ================================================
# Debugging: Check the factor setup
# ================================================

cat("\n=== DEBUGGING: Factor Setup Details ===\n")
print("Factor information:")
print(setup$factors)

print("Decision map:")
print(setup$decision_map)

print("Custom factor groups:")
print(setup$factor_groups)

print("Check if wf_* columns were created in data:")
wf_cols <- grep("^wf_", names(setup$data), value = TRUE)
print(paste("wf_* columns found:", paste(wf_cols, collapse = ", ")))

print("Sample of prepared data (wf_* columns):")
if (length(wf_cols) > 0) {
  print(head(setup$data[, c("study", "es_id", wf_cols)]))
} else {
  print("ERROR: No wf_* columns found in setup$data!")
}

# ================================================
# Expected Output Summary
# ================================================

cat("\n=== EXPECTED BEHAVIOR ===\n")
cat("Risk_of_Bias (decision = 'N'): Should create 3 separate multiverse analyses\n")
cat("Technology (decision = 'U'): Should create multiverse options within each risk group\n") 
cat("Population (decision = 'E'): Should create multiverse options within each risk group\n")
cat("\nTotal expected analyses: 3 risk groups × technology options × population options × methods\n")
```


```{r}
devtools::load_all()

# 1. Prepare data, this needs to be easier
data_validated <- check_data_multiverse(data_tiny)

create_user_friendly_digDep <- function(data_validated) {
  
  # Create a copy and rename the wf_* columns to meaningful names
  user_data <- data_digDep
  
  # Rename wf_* columns to what researchers would actually call them
  names(user_data)[names(user_data) == "wf_1"] <- "technology_type"      # website, mobile, etc.
  names(user_data)[names(user_data) == "wf_2"] <- "guidance_level"       # guided, minimal support, etc.
  names(user_data)[names(user_data) == "wf_3"] <- "population"           # adult, etc.
  names(user_data)[names(user_data) == "wf_4"] <- "therapy_approach"     # cbt-based, not-cbt-based, etc.
  names(user_data)[names(user_data) == "wf_5"] <- "control_type"         # waitlist, other control
  names(user_data)[names(user_data) == "wf_6"] <- "diagnostic_status"    # clinical cutoff, etc.
  names(user_data)[names(user_data) == "wf_7"] <- "risk_of_bias"         # some concerns, high risk, etc.
  names(user_data)[names(user_data) == "wf_8"] <- "time_point"           # post-treatment, follow-up
  
  return(user_data)
}

depression_data <- create_user_friendly_digDep(data_validated)


# 2. prepare which factors
setup <- define_factors(
  depression_data,
  Risk_of_Bias = list("risk_of_bias", 
                      decision = "N", 
                      groups = list(
                        "low_risk_only" = "low risk",
                        "exclude_high_risk" = c("low risk", "some concerns"), 
                        "all_studies" = c("low risk", "some concerns", "high risk")
                      )
  ),
  Technology = list("technology_type", 
                    decision = "U",
                    type = "binary"),
  Population = list("population", 
                    decision = "E",
                    type = "binary")
  
)

# 3. create specifiations
xxx

# 4. run analysis

results <- run_multiverse_analysis(
  data = depression_data,
  specifications = custom_specs$specifications,
  factor_groups = setup$factor_groups,
  verbose = TRUE
)

```

```{r}
#' Enhanced Multiverse Specification Creation
#'
#' Updated to handle custom factor groupings
#'
#' @param factor_setup Result from define_factors()
#' @param methods Which meta-analysis methods to use
#' @param dependencies How to handle multiple effects per study
#' @param min_studies Minimum studies needed for analysis
#'
#' @export
run_multiverse <- function(factor_setup, 
                           methods = c("reml", "fe"),
                           dependencies = "aggregate", 
                           min_studies = 3) {
  
  if (!"factors" %in% names(factor_setup)) {
    stop("Please provide output from define_factors()")
  }
  
  # Validate data first
  cat("Checking your data...\n")
  validated_data <- check_data_multiverse(factor_setup$data)
  
  # Create specifications with custom groupings
  cat("Creating analysis specifications...\n")
  specs <- create_enhanced_multiverse_specifications(
    data = validated_data,
    factor_setup = factor_setup,
    ma_methods = methods,
    dependencies = dependencies
  )
  
  result <- list(
    specifications = specs$specifications,
    factor_info = factor_setup$factors,
    factor_groups = factor_setup$factor_groups,
    n_specs = specs$number_specs,
    n_analyses = length(unique(specs$specifications$multiverse_id))
  )
  
  cat("\n✓ Ready to analyze!\n")
  cat("  - Created", result$n_specs, "specifications\n") 
  cat("  - Across", result$n_analyses, "separate analysis(es)\n")
  cat("\nNext: analyze_multiverse(results) to run all analyses\n")
  
  return(result)
}

#' Create specifications with custom factor groupings
#' @keywords internal
create_enhanced_multiverse_specifications <- function(data, factor_setup, ma_methods, dependencies) {
  
  factor_info <- factor_setup$factors
  factor_groups <- factor_setup$factor_groups
  decision_map <- factor_setup$decision_map
  
  # Build factor combinations considering custom groupings
  wf_factors <- list()
  
  for (i in seq_len(nrow(factor_info))) {
    wf_col <- factor_info$wf_internal[i]
    
    if (factor_info$grouping_type[i] == "simple") {
      # Standard approach
      vals <- unique(data[[wf_col]])
      if (factor_info$decision[i] %in% c("E", "U")) {
        vals <- c(vals, paste0("total_", wf_col))
      }
      wf_factors[[wf_col]] <- vals
      
    } else if (factor_info$grouping_type[i] %in% c("custom", "ordered", "binary")) {
      # Use custom groups
      if (wf_col %in% names(factor_groups)) {
        group_names <- names(factor_groups[[wf_col]])
        wf_factors[[wf_col]] <- group_names
      } else {
        stop("Custom groups not found for ", wf_col)
      }
    }
  }
  
  # Generate the full cartesian grid
  grid_args <- c(wf_factors,
                 list(dependency = dependencies,
                      ma_method = ma_methods))
  specs <- do.call(expand.grid, grid_args)
  
  # Create multiverse IDs based on N-type and C-type factors
  sep_factors <- factor_info$wf_internal[factor_info$decision %in% c("N", "C")]
  if (length(sep_factors) > 0) {
    specs$multiverse_id <- apply(
      specs[, sep_factors, drop = FALSE],
      1, paste, collapse = "|"
    )
  } else {
    specs$multiverse_id <- "all"
  }
  
  # Add row IDs
  specs$row_id <- seq_len(nrow(specs))
  
  # Store custom grouping info as attributes
  attr(specs, "factor_groups") <- factor_groups
  attr(specs, "factor_info") <- factor_info
  
  list(specifications = specs, number_specs = nrow(specs))
}

# =============================================================================
# REALISTIC EXAMPLES
# =============================================================================

#' Example: Risk of Bias Factor
example_risk_of_bias <- function() {
  
  # Sample data
  sample_data <- data.frame(
    study = paste0("Study_", 1:20),
    es_id = 1:20,
    yi = rnorm(20, 0.5, 0.3),
    vi = runif(20, 0.01, 0.05),
    risk_of_bias = sample(c("low risk", "some concerns", "high risk"), 20, replace = TRUE),
    stringsAsFactors = FALSE
  )
  
  cat("=== RISK OF BIAS EXAMPLE ===\n\n")
  
  # Show the realistic approach to risk of bias
  setup <- define_factors(sample_data,
                          Risk_of_Bias = list("risk_of_bias", 
                                              groups = list(
                                                "low_risk_only" = "low risk",
                                                "exclude_high_risk" = c("low risk", "some concerns"), 
                                                "all_studies" = c("low risk", "some concerns", "high risk")
                                              )
                          )
  )
  
  return(setup)
}

#' Example: Ordered Quality Levels  
example_ordered_quality <- function() {
  
  sample_data <- data.frame(
    study = paste0("Study_", 1:15),
    es_id = 1:15,  
    yi = rnorm(15, 0.4, 0.2),
    vi = runif(15, 0.01, 0.04),
    quality = sample(c("high", "medium", "low"), 15, replace = TRUE),
    stringsAsFactors = FALSE
  )
  
  cat("=== ORDERED QUALITY EXAMPLE ===\n\n")
  
  setup <- define_factors(sample_data,
                          Study_Quality = list("quality", 
                                               type = "ordered", 
                                               levels = c("high", "medium", "low")  # High quality studies first
                          )
  )
  
  return(setup)
}
```


## generate_multiverse_report_text
```{r}
report <- generate_multiverse_report_text(
  results = res,
  original_data = data_validated,
  min_k = 5,
  clin_g = 0.24
)
```

```{r}
# User's natural workflow
my_data <- data.frame(
  study = c("Smith2020", "Jones2021", "Brown2019"),
  es_id = 1:3,
  yi = c(0.5, 0.3, 0.7),
  vi = c(0.02, 0.03, 0.025),
  population = c("Adults", "Children", "Adults"),
  design = c("RCT", "Observational", "RCT"),
  stringsAsFactors = FALSE
)
# One-time setup with meaningful labels
wf_setup <- setup_which_factors(
  my_data,
  which_factors = c("Population Type" = "population", 
                    "Study Design" = "design")
)

# Rest of analysis uses internal wf_* columns
validated_data <- check_data_multiverse_enhanced(my_data, wf_setup)

# But plots can use friendly labels
display_labels <- get_display_labels(wf_setup$wf_mapping)
```


report removes bayes due to missing pvalues

```{r}
# Generate full report
report <- generate_multiverse_report(
  results = res,
  specifications = specs,
  original_data = data_tiny,
  output_format = "markdown",  # or "html", "text"
  include_plots = TRUE,
  plot_output_dir = "plots/"   # saves plots as PNG files
)

# Access components
cat(report$report)                    # Print formatted report
report$plots$overall_distribution     # View specific plot
report$summary_stats                  # Raw statistics
report$wf_analysis                    # Which factors analysis

# Create interactive plots
interactive_plots <- create_interactive_multiverse_plots(
  results = res,
  wf_columns = c("wf_population", "wf_measure")
)
```



## Visualize ensamble

```{r}
library(dplyr)
library(ggplot2)
library(viridis)     # for fresh colours

## 0. Prep -------------------------------------------------------------------
plot_dat <- res %>%                 # your multiverse results
  unite("which_label", starts_with("wf_"),
        sep = " · ", remove = FALSE) %>%   # keeps original columns too
  mutate(
    which_label = forcats::fct_inorder(which_label),   # nice ordering
    dependency  = factor(dependency,
                         levels = c("select_max", "select_min",
                                    "aggregate", "modeled")),
    ma_method   = factor(ma_method,  levels = unique(ma_method))
  )  %>%
  mutate(y_row = interaction(ma_method, which_label, sep = "  —  "))

## 1. Stagger the three versions ------------------------------------------------
# position_dodge with a small width nudges points/CIs horizontally
pd <- position_dodge(width = 0.4)

## 2. Build the plot -----------------------------------------------------------
ggplot(plot_dat,
       aes(x      = b,
           y      = y_row,
           colour = dependency,
           group  = dependency)) +
  
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = .3) +
  
  geom_errorbarh(aes(xmin = ci.lb, xmax = ci.ub),
                 height = 0.25, linewidth = .7,
                 position = pd) +
  
  geom_point(size = 3, shape = 18, position = pd) +
  
  scale_colour_viridis_d(begin = .05, end = .85, option = "C") +
  
  labs(x = "Effect size (g)",
       y = NULL,
       colour = "Dependency") +
  
  theme_minimal(base_size = 13) +
  theme(
    panel.grid.major.y = element_blank(),
    legend.position    = "bottom"
  )
```

## Facete by multiverse

```{r}
# 1) Focus on estimator comparison (facets = Which)
ggplot(plot_dat, aes(x = b, y = ma_method, colour = dependency)) +
  facet_grid(rows = vars(which_label), scales = "free_y", space = "free_y") +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = .3) +
  geom_errorbarh(aes(xmin = ci.lb, xmax = ci.ub),
                 height = .25, position = pd) +
  geom_point(size = 3, shape = 18, position = pd) +
  facet_grid(rows = vars(which_label), scales = "free_y", space = "free_y") +
  scale_colour_viridis_d(begin = .05, end = .85, option = "C") +
  labs(x = "Effect size (g)", y = NULL, colour = "Dependency") +
  theme_minimal(base_size = 13) +
  theme(legend.position = "bottom",
        panel.grid.major.y = element_blank())

# 2) Focus on Which-decision comparison (facets = estimator)
ggplot(plot_dat, aes(x = b, y = which_label, colour = dependency)) +
  facet_grid(rows = vars(ma_method), scales = "free_y", space = "free_y") +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = .3) +
  geom_errorbarh(aes(xmin = ci.lb, xmax = ci.ub),
                 height = .25, position = pd) +
  geom_point(size = 3, shape = 18, position = pd) +
  facet_grid(rows = vars(which_label), scales = "free_y", space = "free_y") +
  scale_colour_viridis_d(begin = .05, end = .85, option = "C") +
  labs(x = "Effect size (g)", y = NULL, colour = "Dependency") +
  theme_minimal(base_size = 13) +
  theme(legend.position = "bottom",
        panel.grid.major.y = element_blank())
```

```{r}
ggplot(plot_dat, aes(x = b, y = ma_method, colour = dependency, group = dependency)) +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = .3) +
  geom_errorbarh(aes(xmin = ci.lb, xmax = ci.ub),
                 height = .25, position = pd) +
  geom_point(size = 3, shape = 18, position = pd) +
  facet_grid(rows = vars(which_label), scales = "free_y", space = "free_y") +
  scale_colour_viridis_d(begin = .05, end = .85, option = "C") +
  labs(x = "Effect size (g)", y = NULL, colour = "Dependency") +
  theme_minimal(base_size = 13) +
  theme(legend.position = "bottom",
        panel.grid.major.y = element_blank())
```


# dev: principled ma

# example

```{r}
# Example usage:
data_digDep

decision_map <- c(wf_1="E"
)
ensamble <- makeMethodsList()
out <- create_principled_multiverse_specifications(
  data          = data_digDep,
  wf_vars       = c("wf_1"),
  ma_methods    = ensamble,
  dependencies  = "modeled",
  decision_map  = decision_map
)

specs <- out$specifications
# ▶ specs$cluster_id shows which "N partition" each spec belongs to

# Set globally for your session
options(metaMultiverse.k_smallest_ma = 2)

res <- run_multiverse_analysis(data_multiverse, specs, verbose = T) 


plotly_VoE(res$results)
```


```{r}
set.seed(1)
data_tiny <- metaMultiverse::data_digDep |>
  dplyr::slice_sample(n = 100)

decision_map <- c(wf_1="E", wf_2="N", wf_3="U")

out <- create_principled_multiverse_specifications(
  data          = data_tiny,
  wf_vars       = c("wf_1","wf_2","wf_3"),
  ma_methods    = c("fe","reml"),
  dependencies  = c("aggregate"),
  decision_map  = decision_map
)

specs <- out$specifications
# ▶ specs$cluster_id shows which "N partition" each spec belongs to

# Set globally for your session
options(metaMultiverse.k_smallest_ma = 2)

res <- run_multiverse_analysis(data_tiny, specs, verbose = T) 

res$multiverse_warnings
res$results %>% 
  group_by(multiverse_id) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))
```



```{r}
# example sketch ----------------------------------------------------------
decision_map <- tibble::tribble(
  ~wf_var,         ~level,         ~type,      ~rationale,
  "wf_1",          "website",      "N",        "alternative coding equally valid",
  "wf_1",          "total_wf_1",   "N",        "theory says total score superior",
  "dependency",    "modeled",      "E",        "heterogeneity clearly present",
  "dependency",    "aggregate",    "U",        "precision vs validity trade-off",
  "ma_method",     "reml",         "E",        "standard RE meta",
  "ma_method",     "3-level",      "E",        "accepted hierarchical model"
)
```

```{r}
library(tidyr)

## 0.  Join the decision map -----------------------------------------------
specs2 <- specs %>%                               # from create_multiverse_specifications()
  left_join(decision_map,
            by = c("dependency" = "level")) %>%   # join on whichever columns you mapped
  mutate(type = coalesce(type, "E"))              # default all unmatched rows to "E"

## 1.  Identify which columns are Type-N (principled) -----------------------
# add a flag per column = does this row contain a Type-N choice?
n_flags <- specs2 %>%
  select(starts_with("wf_"), dependency, ma_method) %>%   # the decision columns
  names() %>%
  setNames(., .) %>%                     # keep names for map
  lapply(function(col)
    specs2$type == "N" & !is.na(specs2[[col]])) %>%
  bind_cols() %>%
  mutate(set_id = group_indices(., !!!syms(names(.))))    # same values -> same set

specs2 <- bind_cols(specs2, n_flags["set_id"])

## 2.  Split by principled set, run multiverse, and bind --------------------
library(purrr)

results <- specs2 %>%
  split(.$set_id) %>%
  map_dfr(~ run_multiverse_analysis(
    data_multiverse = data_digDep,
    specifications  = .x))
```



# Optimized R Package Development Workflow

## 1. Primary Development Cycle (Daily Work)

### Use this workflow when actively developing functions and making frequent changes.

```{r}
# Load your package functions without full installation (fast!)
devtools::load_all()

# view functions
ls(getNamespace("metaMultiverse"))

# Test specific functions you're working on
testthat::test_file("../tests/testthat/test-check_data_multiverse.R")

# Interactive testing in console
# my_function(test_arguments)
```
When to use: During active development of functions, several times per hour/day.

## 2. Documentation & Integration Phase

### Use this workflow when you've completed features and need to update documentation and ensure everything works together.
```{r}
# Generate documentation from roxygen comments
devtools::document()

# Run all tests to ensure nothing broke
devtools::test()

# Quick check (faster than full check)
#devtools::check(document = FALSE, args = c("--no-manual", "--as-cran"))
```
When to use: After completing a feature, once or twice per day.

<br>

## 3. Release Preparation Phase
### Use this workflow when preparing to share your package with others or submit to CRAN.

```{r}
# Clean compiled code for fresh build
devtools::clean_dll()
pkgbuild::clean_dll()

# Make sure NAMESPACE is properly generated
devtools::document()

# Build the package
devtools::build()

# Install locally
devtools::install()

# Run comprehensive checks
devtools::check(manual = TRUE, cran = TRUE)
```

When to use: Before sharing your package, submitting to CRAN, or creating a GitHub release.

<br>

## 4. Full Reset (Occasional Use)
### Only use when experiencing strange behavior that might be due to cached objects or old builds.
```{r}
# Clear R environment
rm(list = ls())

# Manually delete the NAMESPACE file if necessary
unlink("../NAMESPACE") 

# Update installed packages
update.packages(ask = FALSE, checkBuilt = TRUE)

# Clean compiled code
devtools::clean_dll()
pkgbuild::clean_dll()

# Rebuild everything
devtools::document()
devtools::build()
devtools::install()
devtools::load_all()
devtools::test()
devtools::check()
```



## Previous Development Workflow
```{r}
# # Load devtools and set up the working directory
# ##library(devtools)
# ##setwd("../")  # Set working directory to the package root
# rm(list = ls())
# update.packages(ask = FALSE, checkBuilt = TRUE)
# unlink("../NAMESPACE") # Manually delete the NAMESPACE file from the package directory
# devtools::clean_dll()    # Clean compiled objects
# pkgbuild::clean_dll()
# ##usethis::use_mit_license("Constantin Yves Plessen")
# devtools::document()
# devtools::build()
# devtools::install()    # Install the package
# devtools::load_all()
# devtools::test()
# devtools::check()
```

```{r}
example(plotly_VoE, package = "metaMultiverse")
```

```{r}
example(run_multiverse_analysis, package = "metaMultiverse")
```

```{r}
usethis::edit_r_environ()
#Add _R_CHECK_SYSTEM_CLOCK_=0 to the file
```



1.	Install Missing Packages: Install Matrix, KernSmooth, and any other missing dependencies.


2.	Fix Undefined Variables: Ensure factor_label_lookup is properly defined and passed in your code and tests.

3.	Add Imports: Ensure all external packages (e.g., dplyr, ggplot2, viridis) are imported using @importFrom in roxygen2 comments and that DESCRIPTION lists them correctly.

4.	Handle Global Variables: Use globalVariables() to declare global variables that are not directly passed as arguments.

5.	Fix Example Code: Ensure assign_quantile_levels is correctly applied to the example data frame.

6.	Rerun devtools::document(): Update the documentation and NAMESPACE file by rerunning devtools::document() after fixing the issues above.

7.	Run R CMD check Again: After fixing the issues, run R CMD check to ensure all warnings and errors are resolved.

W  checking dependencies in R code ...
'::' or ':::' imports not declared from:
‘RColorBrewer’ ‘cowplot’ ‘dplyr’ ‘ggplot2’ ‘glue’ ‘metafor’ ‘plotly’
‘puniform’ ‘purrr’ ‘scales’ ‘stringr’ ‘tidyr’ ‘viridis’


```{r}
# Load roxygen2
library(roxygen2)

# Preview documentation for specific object
?`data_digDep`  # With backticks
```



# 1. Function Development

## Step 1. Validate Data with Data Validator

### Load Data
```{r}
digDep_data_multiverse <- read_csv("~/Documents/work/1 projects/4 multiverse projects/multiverse-meta-analysis-depression/data/tidy/digDep_multiverse_data.csv")

## Select required variables and create example data set
data_digDep <- digDep_data_multiverse %>% 
  select(study,
         es_id,
         yi,
         vi,
         sei,
         wf_1:wf_8,
         condition_arm1,
         condition_arm2
  )

data_digDep %>% head() %>%  dput()
```


```{r}
# Create a data directory if it doesn't exist
dir.create("data", showWarnings = FALSE)

# Save the dataset
usethis::use_data(data_digDep, overwrite = TRUE)
```


### `check_data_multiverse`
```{r}
check_data_multiverse <- function(data) {
  # Required columns
  required_columns <- c(
    "study", "es_id", "yi", "vi", "sei",
    "condition_arm1", "condition_arm2"
  )
  
  # Dynamically detect wf columns (e.g., wf_1, wf_2, ...)
  wf_columns <- grep("^wf_", colnames(data), value = TRUE)
  
  # Combine required and dynamically detected wf columns
  all_required_columns <- c(required_columns, wf_columns)
  
  # Check if all required columns are present
  missing_columns <- setdiff(all_required_columns, colnames(data))
  if (length(missing_columns) > 0) {
    stop(paste("Missing required columns:", paste(missing_columns, collapse = ", ")))
  }
  
  # Check data types
  expected_types <- list(
    study = "character",
    es_id = "numeric",
    yi = "numeric",
    vi = "numeric",
    sei = "numeric",
    condition_arm1 = "character",
    condition_arm2 = "character"
  )
  
  type_mismatches <- purrr::map_lgl(names(expected_types), function(col) {
    if (!inherits(data[[col]], expected_types[[col]])) {
      message(sprintf("Column '%s' has incorrect type. Expected: %s", col, expected_types[[col]]))
      return(TRUE)
    }
    FALSE
  })
  
  if (any(type_mismatches)) {
    stop("One or more columns have incorrect data types. See messages above.")
  }
  
  # Check for missing values
  missing_values <- colSums(is.na(data[all_required_columns]))
  if (any(missing_values > 0)) {
    warning("Some required columns contain missing values:\n", 
            paste(names(missing_values[missing_values > 0]), 
                  missing_values[missing_values > 0], 
                  sep = ": ", collapse = "\n"))
  }
  
  # Check unique es_id
  if (anyDuplicated(data$es_id)) {
    stop("Duplicate values found in 'es_id'. Each effect size must have a unique identifier.")
  }
  
  # Dynamic validation for wf variables (e.g., categorical levels)
  wf_checks <- purrr::map(wf_columns, function(wf_col) {
    if (!is.character(data[[wf_col]])) {
      message(sprintf("Column '%s' should be a character variable.", wf_col))
      return(FALSE)
    }
    TRUE
  })
  
  if (!all(unlist(wf_checks))) {
    stop("One or more 'wf_' columns have incorrect data types.")
  }
  
  # Success message if all checks pass
  message("Data check passed. Dataset is valid.")
  return(TRUE)
}
```

### Inspect data 

```{r}
glimpse(data)
```

```{r}
check_data_multiverse(data)
```


## Step 2. Create specifications

### `create_multiverse_specifications`
```{r}
# Function to create specifications for multiverse meta-analysis
create_multiverse_specifications <- function(data, wf_vars, ma_methods, dependencies) {
  # Input validation
  if (!is.data.frame(data)) stop("`data` must be a data frame.")
  if (!all(wf_vars %in% colnames(data))) stop("Not all `wf_vars` are present in the dataset.")
  
  # Create "Which factors"
  wf_factors <- lapply(wf_vars, function(wf) {
    unique_values <- unique(data[[wf]])
    c(unique_values, paste0("total_", wf))
  })
  
  names(wf_factors) <- wf_vars
  
  # Validate "How factors"
  if (length(ma_methods) == 0) stop("`ma_methods` must include at least one method.")
  if (length(dependencies) == 0) stop("`dependencies` must include at least one dependency.")
  
  # Dynamically build the expand.grid call
  grid_args <- c(wf_factors, list(dependency = dependencies, ma_method = ma_methods))
  
  # Generate the specifications grid
  specifications_grid <- do.call(expand.grid, grid_args)
  
  
  # Prune impossible paths
  specifications_grid <- specifications_grid %>%
    filter((dependency == "modeled" & 
              ma_method %in% c("3-level", "rve")) | 
             (dependency == "aggregate" & 
                ma_method %in% c("reml", "fe", "p-uniform", "pet-peese", "uwls", "waap")) | 
             (dependency == "ignore" & 
                ma_method %in% c("reml", "fe"))) %>% 
    mutate(row_id = row_number())
  
  # Return final specifications grid
  specifications <- data.frame(specifications_grid)
  number_specs <- nrow(specifications)
  
  list(
    specifications = specifications,
    number_specs = number_specs
  )
}
```

#### Setup
```{r}
# Example dataset
data_multiverse <- data  # Replace with your actual dataset

# Define "Which factors"
wf_vars <- paste0("wf_", 1:8)

# Lookup table for WF labels
wf_label_lookup <- list(
  wf_1 = "Technology",
  wf_2 = "Guidance",
  wf_3 = "Target Group",
  wf_4 = "Therapy Type",
  wf_5 = "Control Group",
  wf_6 = "Diagnosis",
  wf_7 = "Risk of Bias",
  wf_8 = "Time Point"
)

# Define "How factors"
ma_methods <- c("reml", 
                "fe", 
                "p-uniform", 
                "pet-peese", 
                "pet-peese (corrected)", 
                "uwls", 
                "waap", 
                "3-level", 
                "rve")

dependencies <- c("ignore", "aggregate", "modeled")
```

### Run `create_multiverse_specifications`
```{r}
# Generate specifications
results <- create_multiverse_specifications(
  data_multiverse, 
  wf_vars, 
  ma_methods, 
  dependencies)

specifications <- results$specifications
```


## Step 3. Run Multiverse

### `general_multiverse`

```{r}
general_multiverse <- function(i, data_multiverse, specifications, how_methods) {
  # Prepare output
  out <- list()
  dat <- as.data.frame(data_multiverse)
  
  # Apply "Which" factors dynamically
  for (wf in names(specifications)[grepl("^wf_", names(specifications))]) {
    filter_value <- specifications[[wf]][i]
    if (!grepl("^total_", filter_value)) {
      dat <- dat[dat[[wf]] == filter_value, ]
    }
  }
  
  # Remove rows with missing `yi` or `vi` values
  dat <- tidyr::drop_na(dat, dplyr::any_of(c("yi", "vi")))
  set <- paste(dat$es_id, collapse = ",")
  
  # Only compute if at least 2 unique studies
  if (length(unique(dat$study)) < 2) {
    return(NULL)
  }
  
  # Handle "How" factors dynamically
  dependency <- specifications$dependency[i]
  ma_method <- specifications$ma_method[i]
  
  # Ignoring dependency
  if (dependency == "ignore") {
    mod <- run_ignore_dependency(dat, ma_method, how_methods)
  }
  
  # Aggregating dependency
  else if (dependency == "aggregate") {
    mod <- run_aggregate_dependency(dat, ma_method, how_methods)
  }
  
  # Modeling dependency
  else if (dependency == "modeled") {
    mod <- run_modeled_dependency(dat, ma_method, how_methods)
  }
  
  # Return results
  out <- data.frame(
    specifications[i, ],
    b = mod$b[[1]],
    ci.lb = mod$ci.lb[[1]],
    ci.ub = mod$ci.ub[[1]],
    pval = mod$pval[[1]],
    k = nrow(dat),
    set
  )
  
  return(out)
}
```


### Meta-Analytical Models

#### `calculate_puni_star`

```{r}
calculate_puni_star <- function(dat) {
  mod <- tryCatch({
    mod.puni <- puniform::puni_star(
      yi = dat$yi, 
      vi = dat$vi, 
      side = "right"
    )
    
    # Return a standardized list format
    list(
      b = mod.puni$est, 
      pval = mod.puni$pval.0, 
      ci.lb = mod.puni$ci.lb, 
      ci.ub = mod.puni$ci.ub
    )
  }, error = function(e) {
    # Gracefully handle errors
    list(
      b = NA, 
      pval = NA, 
      ci.lb = NA, 
      ci.ub = NA
    )
  })
  
  return(mod)
}
```

#### `calculate_pet.peese`
```{r}
calculate_pet.peese <- function(data) {
  mod <- list()
  
  # Try PET estimation
  pet_fit <- tryCatch({
    lm(yi ~ sqrt(vi), weights = 1 / vi, data = data)
  }, error = function(e) return(NULL))
  
  if (is.null(pet_fit)) {
    # PET estimation failed
    return(list(b = NA, ci.lb = NA, ci.ub = NA, pval = NA, type = "PET (failed)"))
  }
  
  # Extract PET p-value
  pet_p <- tryCatch({
    coef(summary(pet_fit))["(Intercept)", "Pr(>|t|)"]
  }, error = function(e) return(NA))
  
  if (is.na(pet_p) || pet_p >= 0.1) {
    # PET is valid or PEESE is not triggered
    mod <- tryCatch({
      list(
        b = coef(summary(pet_fit))["(Intercept)", "Estimate"],
        ci.lb = confint(pet_fit)["(Intercept)", "2.5 %"],
        ci.ub = confint(pet_fit)["(Intercept)", "97.5 %"],
        pval = pet_p,
        type = "PET"
      )
    }, error = function(e) {
      list(b = NA, ci.lb = NA, ci.ub = NA, pval = NA, type = "PET (failed)")
    })
  } else {
    # PEESE estimation
    peese_fit <- tryCatch({
      lm(yi ~ vi, weights = 1 / vi, data = data)
    }, error = function(e) return(NULL))
    
    if (is.null(peese_fit)) {
      # PEESE estimation failed
      return(list(b = NA, ci.lb = NA, ci.ub = NA, pval = NA, type = "PEESE (failed)"))
    }
    
    # Extract PEESE estimates
    mod <- tryCatch({
      list(
        b = coef(summary(peese_fit))["(Intercept)", "Estimate"],
        ci.lb = confint(peese_fit)["(Intercept)", "2.5 %"],
        ci.ub = confint(peese_fit)["(Intercept)", "97.5 %"],
        pval = coef(summary(peese_fit))["(Intercept)", "Pr(>|t|)"],
        type = "PEESE"
      )
    }, error = function(e) {
      list(b = NA, ci.lb = NA, ci.ub = NA, pval = NA, type = "PEESE (failed)")
    })
  }
  
  return(mod)
}
```

#### `add_pet_peese_corrected`
```{r}
# Step 1: Duplicate and correct PET-PEESE rows
add_pet_peese_corrected <- function(data) {
  # Identify and correct PET-PEESE results
  pet_peese_corrected <- data %>%
    filter(ma_method == "pet-peese" & b < 0) %>%  # Only keep rows where b < 0
    mutate(b = 0,  # Correct negative effect size estimates
           ma_method = "pet-peese (corrected)")  # Update method label
  
  # Combine the corrected results with the original data
  data_combined <- bind_rows(data, pet_peese_corrected)
  
  # Reset factor levels to ensure no duplicates
  data_combined$ma_method <- factor(data_combined$ma_method, levels = unique(data_combined$ma_method))
  
  return(data_combined)
}
```


#### `calculate_uwls`
```{r}
# Function to calculate UWLS
calculate_uwls  <- function(dat) {
  mod <- list()
  d = dat$yi
  sed = sqrt(dat$vi) 
  t = d/sed
  Precision=1/sed
  reg_uwls = lm(t ~ 0 + Precision)
  mod$pval <- coef(summary(reg_uwls))["Precision", "Pr(>|t|)"] # pet p-value < .10 -> peese
  mod$b  <- coef(summary(reg_uwls))["Precision", "Estimate"] # peese estimate
  mod$ci.lb <- confint(reg_uwls)["Precision", "2.5 %"] 
  mod$ci.ub <- confint(reg_uwls)["Precision", "97.5 %"] 
  
  return(mod)
}
```


#### `calculate_waap`
```{r}
# Function to calculate WAAP
calculate_waap <- function(dat) {
  mod <- list()
  d = dat$yi
  sed = sqrt(dat$vi) 
  k = length(d) #number of studies
  t = d/sed
  Precision=1/sed
  reg_uwls = lm(t ~ 0 + Precision)
  UWLS <- as.numeric(reg_uwls$coefficients)
  powered<-sed<abs(UWLS)/2.8
  if(sum(powered)<2) {
    mod$pval  <- NA
    mod$b     <- NA
    mod$ci.lb <- NA
    mod$ci.ub <- NA
  } else{
    reg_waap=lm(t[powered]~ 0 + Precision[powered])
    mod$pval  <- coef(summary(reg_waap))["Precision[powered]", "Pr(>|t|)"] 
    mod$b     <- coef(summary(reg_waap))["Precision[powered]", "Estimate"] 
    mod$ci.lb <- confint(reg_waap)["Precision[powered]", "2.5 %"] 
    mod$ci.ub <- confint(reg_waap)["Precision[powered]", "97.5 %"] 
  }
  return(mod)
}

```


### `run_aggregate_dependency`
```{r}
run_aggregate_dependency <- function(dat, ma_method, how_methods) {
  # Calculate effect sizes
  dat <- metafor::escalc(yi = yi, vi = vi, data = dat)
  
  # Aggregate data by cluster (study)
  dat <- metafor::aggregate.escalc(
    dat, 
    cluster = study, 
    struct = "CS",  # Compound symmetric structure
    rho = 0.5
  )
  
  # Run models based on method
  if (ma_method == "fe" & "fe" %in% how_methods) {
    return(metafor::rma(yi = dat$yi, vi = dat$vi, method = "FE"))
  } else if (ma_method == "reml" & "reml" %in% how_methods) {
    return(metafor::rma(yi = dat$yi, vi = dat$vi, method = "REML", control = list(stepadj = 0.5, maxiter = 2000)))
  } else if (ma_method == "uwls" & "uwls" %in% how_methods) {
    return(calculate_uwls(dat))
  } else if (ma_method == "waap" & "waap" %in% how_methods) {
    return(calculate_waap(dat))
  } else if (ma_method == "pet-peese" & "pet-peese" %in% how_methods) {
    return(calculate_pet.peese(dat))
  } else if (ma_method == "p-uniform" & "p-uniform" %in% how_methods) {
    return(calculate_puni_star(dat))
  } else {
    return(list(b = NA, pval = NA, ci.lb = NA, ci.ub = NA))
  }
}
```

### `run_ignore_dependency`
```{r}
run_ignore_dependency <- function(dat, ma_method, how_methods) {
  if (ma_method == "fe" & "fe" %in% how_methods) {
    return(metafor::rma(yi = dat$yi, vi = dat$vi, method = "FE"))
  } else if (ma_method == "reml" & "reml" %in% how_methods) {
    return(metafor::rma(yi = dat$yi, vi = dat$vi, method = "REML", control = list(stepadj = 0.5, maxiter = 2000)))
  } else {
    list(b = NA, pval = NA, ci.lb = NA, ci.ub = NA)
  }
}
```

### `run_modeled_dependency`
```{r}
# Helper function for "Modeled Dependency"
run_modeled_dependency <- function(dat, ma_method, how_methods) {
  if (sum(duplicated(dat$study)) > 1) {
    # Try to fit the model
    mod_modeled <- tryCatch({
      metafor::rma.mv(
        data = dat, yi = yi, V = vi, method = "REML",
        random = list(~1 | es_id, ~1 | study), sparse = TRUE
      )
    }, error = function(e) list(error = TRUE))  # Add an error indicator
    
    # Check if the model fitting was successful
    if (!is.list(mod_modeled) || is.null(mod_modeled$error)) {
      if (ma_method == "3-level" & "3-level" %in% how_methods) {
        return(mod_modeled)
      } else if (ma_method == "rve" & "rve" %in% how_methods) {
        return(tryCatch({
          metafor::robust(mod_modeled, 
                          cluster = dat$study, 
                          clubSandwich = TRUE)
        }, error = function(e) list(b = NA, pval = NA, ci.lb = NA, ci.ub = NA)))
      }
    }
  }
  
  # Default return if no valid model or unsupported method
  list(b = NA, pval = NA, ci.lb = NA, ci.ub = NA)
}
```

### `run_multiverse_analysis`
```{r}
run_multiverse_analysis <- function(data_multiverse, specifications, how_methods) {
  # Run multiverse for each specification
  results <- lapply(seq_len(nrow(specifications)), function(i) {
    general_multiverse(i, data_multiverse, specifications, how_methods)
  })
  
  # Combine results into a single data frame
  final_results <- do.call(rbind, results)
  
  # Add full_set indicator
  final_results$full_set <- as.numeric(final_results$set == paste(1:nrow(data_multiverse), collapse = ","))
  
  # Remove missing values
  final_results <- final_results[complete.cases(final_results), ]
  
  # Remove duplicates, keeping first occurrence (more specific which factors)
  final_results <- final_results[!duplicated(final_results[, c("b", "set", "ma_method")]), ]
  
  if ("pet-peese (corrected)" %in% how_methods) {
    final_results <- add_pet_peese_corrected(final_results)
  }
  return(final_results)
}
```


### Test `run_multiverse_analysis`

#### Select Data Set
```{r}
set.seed(42)
specifications_test_all <- specifications %>% 
  filter(
    #dependency == "aggregate" & 
    wf_1 == "total_wf_1" & 
      wf_2 == "total_wf_2" & 
      wf_3 == "total_wf_3"& 
      wf_4 == "total_wf_4" & 
      wf_5 == "total_wf_5" & 
      wf_6 == "total_wf_6" & 
      wf_7 == "total_wf_7" & 
      wf_8 == "total_wf_8")

specifications_test_sample <- specifications %>% 
  sample_n(2000)

specifications_test <- specifications_test_all %>% bind_rows(specifications_test_sample)

# specifications <- specifications_test
```

#### Run `run_multiverse_analysis`
```{r}
res <- run_multiverse_analysis(data_multiverse, 
                               specifications, 
                               how_methods = ma_methods)
res %>% glimpse()
```


## Step 4. Data Visualisation

### Spec Curve

```{r}
# Flexible lookup table for WF and How factors
factor_label_lookup <- list(
  wf_1 = "Technology",
  wf_2 = "Guidance",
  wf_3 = "Target Group",
  wf_4 = "Therapy Type",
  wf_5 = "Control Group",
  wf_6 = "Diagnosis",
  wf_7 = "Risk of Bias",
  wf_8 = "Time Point",
  ma_method = "Meta-Analysis Method"  # Include ma_method explicitly
)
```

#### `generate_dynamic_labels`
```{r}
generate_dynamic_labels <- function(data, lookup_table) {
  # Detect wf_ columns and include ma_method
  factor_cols <- c(
    colnames(data)[grep("^wf_", colnames(data))],
    "ma_method"
  )
  
  # Map columns to human-readable labels
  factor_labels <- sapply(factor_cols, function(col) {
    if (!is.null(lookup_table[[col]])) {
      lookup_table[[col]]  # Use lookup table if match exists
    } else {
      col  # Fallback to original column name
    }
  }, USE.NAMES = TRUE)
  
  return(factor_labels)
}
```

#### `generate_tooltip`
```{r}
# Helper Function to Dynamically Generate Tooltip Text
generate_tooltip <- function(data, factor_label_lookup) {
  # Select all columns starting with "wf_"
  wf_columns <- grep("^wf_", colnames(data), value = TRUE)
  
  # Helper function to split long 'set' strings into multiple lines
  split_set_values <- function(set_string, chunk_size = 10) {
    set_values <- unlist(strsplit(set_string, ","))  # Split by commas
    chunks <- split(set_values, ceiling(seq_along(set_values) / chunk_size))  # Split into groups of 10
    formatted_set <- paste(sapply(chunks, paste, collapse = ","), collapse = "<br>")
    return(formatted_set)
  }
  
  # Dynamically create tooltip text
  data$tooltip <- apply(data, 1, function(row) {
    wf_text <- paste(sapply(wf_columns, function(col) {
      value <- as.character(row[col])
      label <- factor_label_lookup[[col]]
      
      # Replace "total_" values with "Combined"
      if (grepl("total_", value)) {
        paste0(label, ": Combined")
      } else {
        paste0(label, ": ", str_to_sentence(value))
      }
    }), collapse = "<br>")
    
    # Format the study set
    set_formatted <- split_set_values(as.character(row["set"]))
    
    # Construct the final tooltip text
    paste0(
      wf_text,
      "<br><b>Mean:</b> ", round(as.numeric(row["b"]), 3),
      "<br><b>CI:</b> [", round(as.numeric(row["ci.lb"]), 3), ", ", round(as.numeric(row["ci.ub"]), 3), "]",
      "<br><b>Studies (k):</b> ", row["k"],
      "<br><b>Study set (ID):</b> ", set_formatted
    )
  })
  
  return(data)
}
```
#### `assign_quantile_levels`
```{r}
# Function to assign quantile-based levels
assign_quantile_levels <- function(data, k_column, num_levels = 9) {
  # Ensure k is numeric
  if (!is.numeric(data[[k_column]])) stop("`k_column` must be numeric.")
  
  # Dynamically adjust breaks for unique quantiles
  fill_quantiles <- unique(quantile(data[[k_column]], probs = seq(0, 1, length.out = num_levels + 1), na.rm = TRUE))
  
  while (length(fill_quantiles) <= num_levels) {
    num_levels <- num_levels - 1
    fill_quantiles <- unique(quantile(data[[k_column]], probs = seq(0, 1, length.out = num_levels + 1), na.rm = TRUE))
    if (num_levels < 2) stop("Insufficient unique values in `k` to create quantile levels.")
  }
  
  # Assign quantile levels
  data$fill_levels <- cut(
    data[[k_column]],
    breaks = fill_quantiles,
    include.lowest = TRUE,
    labels = 1:num_levels
  )
  
  return(data)
}
```


#### `plotly_descriptive_spec_curve`

```{r}
plotly_descriptive_spec_curve <- function(data, 
                                          ylim_lower = NULL, 
                                          ylim_upper = NULL,
                                          colorblind_friendly = TRUE) {
  
  
  # Step 1: Detect WF and How Factors dynamically
  wf_cols <- colnames(data)[grep("^wf_", colnames(data))]
  how_cols <- colnames(data)[colnames(data) %in% c("ma_method")]
  all_factors <- c(wf_cols, how_cols)
  
  # Step 2: Generate human-readable labels
  factor_labels <- generate_dynamic_labels(data, factor_label_lookup)
  
  # Step 3: Combine human-readable labels with unique column components
  factor_levels <- lapply(all_factors, function(col) {
    # Replace 'total_' values with "Combined"
    unique_values <- unique(data[[col]])
    replaced_values <- ifelse(grepl("total_", unique_values), 
                              "Combined", 
                              str_to_sentence(unique_values))
    
    paste0(factor_labels[[col]], ": ", replaced_values)
  })
  
  # Flatten the list and reverse for plotting
  yvar <- factor(
    rev(unlist(factor_levels)), 
    levels = rev(unlist(factor_levels))
  )
  
  x_rank <- rank(data$b, ties.method = "random")
  
  # Generate x-variable = specific meta-analysis (repeated for yvar length)
  xvar <- rep(x_rank, each = length(levels(yvar)))
  
  # Select relevant columns for plotting
  data <- data %>%
    select(all_of(all_factors), b, ci.lb, ci.ub, pval, k, set)
  
  # Generate spec matrix with updated factor levels
  spec <- NULL
  for (i in 1:nrow(data)) {
    id <- as.numeric(yvar %in% 
                       unlist(lapply(all_factors, function(col) {
                         value <- str_to_sentence(data[[col]][i])
                         
                         # Replace if 'total_' is in the value
                         if (grepl("total_", data[[col]][i])) {
                           paste0(factor_labels[[col]], ": Combined")
                         } else {
                           paste0(factor_labels[[col]], ": ", value)
                         }
                       })))
    spec <- c(spec, id)
  }
  
  # Prepare plot data
  plotdata <- data.frame(xvar, yvar, spec)
  
  # Add k values and assign fill
  plotdata$k <- rep(data$k, each = length(levels(yvar)))
  plotdata$fill<- as.factor(plotdata$k * plotdata$spec)
  
  # Coloring Step 1: Assign quantile levels using the helper function
  data <- assign_quantile_levels(data, k_column = "k", num_levels = 9)
  
  # Coloring Step 2: Choose color palette based on argument
  if (colorblind_friendly) { 
    palette_colors <- viridis(length(levels(data$fill_levels)), option = "C", direction = -1) # direction -1 implies darker = more evidence
  } else { 
    palette_colors <- RColorBrewer::brewer.pal(length(levels(data$fill_levels)), "Spectral") # warmer = more evidence, rev() can be used to reverse order
  }
  
  # Add `fill_manual` to data
  data <- data %>%
    mutate(fill_manual = as.factor(fill_levels))
  
  # Apply the same colors to plotdata
  plotdata <- plotdata %>%
    left_join(data %>% select(k, fill_manual) %>% distinct(), by = "k") 
  
  # Only if spec occurs a color is allowed, 0 is white
  plotdata <- plotdata %>%
    mutate(fill_manual = ifelse(spec == 1, fill_manual, 0),
           fill_manual = as.factor(fill_manual)) 
  
  ## === TILE PLOT === ##
  tile_plot <- ggplot(data = plotdata,
                      aes(x = xvar, 
                          y = as.factor(yvar), 
                          fill = fill_manual,
                          text = yvar))+
    geom_raster() +
    scale_x_continuous(position = "bottom") +
    scale_y_discrete(labels = levels(yvar)) +
    scale_fill_manual(values = c("white", palette_colors)) +
    labs(x = "Specification Number", 
         y = "Which/How Factors") +
    theme_classic() +
    theme(legend.position = "none",
          axis.text.y = element_text(colour = "black", size = 8),
          axis.text.x = element_text(colour = "black"),
          axis.ticks = element_line(colour = "black"),
          plot.margin = margin(t = 5.5, 
                               r = 5.5, 
                               b = 5.5, 
                               l = 5.5, 
                               unit = "pt")) +
    coord_cartesian(
      expand = F, 
      xlim = c(0, nrow(data)))
  
  ## === FOREST PLOT === ##
  
  ## Create Limits
  yrng <- range(c(0, data$ci.lb, data$ci.ub))
  
  ## Use user-specified limits if provided
  if (!is.null(ylim_lower)) {
    yrng[1] <- ylim_lower
  }
  if (!is.null(ylim_upper)) {
    yrng[2] <- ylim_upper
  }
  
  # Generate tooltips dynamically
  data <- generate_tooltip(data, factor_label_lookup)
  
  data <- data%>% # for line type in plotly
    mutate(group_id = 1)
  
  spec_curve_plot <- data %>%
    ggplot(aes(x = x_rank, 
               y = b,
               text = tooltip,
               group = group_id)) +
    geom_line(col = "black", linewidth = .2) +
    geom_errorbar(aes(ymin = ci.lb, ymax = ci.ub, col = as.factor(fill_manual)), width = 0.1, size = 0.6) +
    geom_point(size = .2) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
    labs(y = "Summary Effect", x = "") +
    scale_color_manual(values = palette_colors) +
    coord_cartesian(ylim = yrng,
                    xlim = c(0, nrow(data)), 
                    expand = FALSE) +
    theme_bw() +
    theme(legend.position = "none",
          axis.text.x = element_blank(), 
          axis.ticks.x = element_blank())
  
  ## === MAKE PLOTS INTERACTIVE === ##
  spec_curve_plotly <- plotly::ggplotly(spec_curve_plot, 
                                        tooltip = "text")  %>%
    plotly::style(hoverlabel = list(bgcolor = "white", font = list(color = "black")))
  
  tile_plot_plotly <- plotly::ggplotly(tile_plot, tooltip = "text")
  
  # Combine using subplot
  p <- plotly::subplot(spec_curve_plotly, tile_plot_plotly, nrows = 2, shareX = TRUE, titleY = TRUE) 
  p
}
```

# Here
inspect res
```{r}
problematic_rows <- res %>%
  filter(ma_method %in% c("uwls", "waap", "fe")) %>%
  filter(set == "111,231,232" & wf_1 == "mobile")

print(problematic_rows)
# View problematic rows
print(res_filtered) 
#

similar_results <- res %>%
  group_by(set, wf_1, wf_2, wf_3, wf_4, wf_5, wf_6, wf_7, wf_8) %>%
  filter(n_distinct(b) != 1) %>%  # Only keep groups with identical results
  arrange(set)

```

#### Test `plotly_descriptive_spec_curve`
```{r}
# Generate plot
plotly_descriptive_spec_curve(res, ylim_upper = 2, ylim_lower = -2, colorblind_friendly = F)
```

### Vibrations of Effects

#### `compute_density_colors`
```{r}
# Function to compute density-based coloring
compute_density_colors <- function(data, x, y, colorblind_friendly = TRUE) {
  dens_values <- densCols(
    x = data[[x]],
    y = data[[y]],
    colramp = if (colorblind_friendly) colorRampPalette(viridis::viridis(256, 
                                                                         direction = -1)) else colorRampPalette(rev(rainbow(10, end = 4/6)))
  )
  data$density <- dens_values
  return(data)
}
```

#### `generate_tooltip`
```{r}
# Function to generate tooltips
generate_tooltip <- function(data, x, y, k_col = "k", set_col = "set") {
  data %>%
    mutate(
      tooltip = glue::glue(
        "<b>Effect Size (b):</b> {round(b, 3)}<br>",
        "<b>P-value:</b> {scales::scientific(pval, digits = 8)}<br>",
        "<b>Number of Studies:</b> {get(k_col)}<br>",
        "<b>Study Set:</b> {stringr::str_wrap(get(set_col), width = 30)}"
      )
    )
}
```

#### `plotly_VoE`
```{r}
# Interactive VoE Plotly Function
plotly_VoE <- function(
    data,
    x = "b",  
    y = "pvalue",  
    colorblind_friendly = TRUE,
    cutoff = 10,
    x_breaks = seq(-0.5, 2, by = 0.25),
    y_breaks = c(1e-11, 1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 0.001, 0.01, 0.05, 0.1, 0.5, 1),
    x_limits = c(-0.7, 2.1),
    y_limits = c(1e-11, 1),
    vertical_lines = c(0.1, 0.9),
    hline_value = 0.05,
    title_template = "{k} meta-analyses with at least {cutoff} studies."
) {
  
  # Replace extreme p-values and filter based on cutoff
  data <- data %>%
    mutate(!!y := ifelse(!!sym(y) < 1e-11, 1e-11, !!sym(y))) %>%
    filter(k >= cutoff)
  
  # Compute density-based colors
  data <- compute_density_colors(data, x, y, colorblind_friendly)
  
  # Add tooltips
  data <- generate_tooltip(data, x, y)
  
  # Number of analyses
  k <- nrow(data)
  
  x_quantiles <- quantile(data[[x]], vertical_lines, na.rm = TRUE)  # Compute quantiles
  
  # Base ggplot
  p <- ggplot(data, aes_string(x = x, y = y, text = "tooltip")) +
    geom_jitter(aes(colour = density), size = 1, width = 0.001,
                show.legend = FALSE) +
    scale_color_identity() +
    theme_bw() +
    geom_hline(yintercept = hline_value, linetype = 2, color = "black") +
    geom_vline(xintercept = quantile(data[[x]], 
                                     vertical_lines, 
                                     na.rm = TRUE), 
               color = "red", 
               linetype = 2) +
    scale_y_continuous(
      trans = "log",
      breaks = y_breaks,
      labels = scales::label_scientific(),
      limits = y_limits
    ) +
    scale_x_continuous(
      breaks = x_breaks,
      limits = x_limits
    ) +
    labs(
      x = "Effect Size (b)",
      y = "P-value",
      title = glue(title_template, k = k, cutoff = cutoff)
    ) +
    theme(
      panel.border = element_blank(),
      plot.title = element_text(size = 10)
    ) +
    guides(colour = "none")
  
  # Convert to interactive Plotly plot
  p_plotly <- ggplotly(p, tooltip = "text") %>%
    layout(hoverlabel = list(bgcolor = "white", font = list(color = "black")))
  
  return(p_plotly)
}
```

#### Test `plotly_VoE`

```{r}
plotly_VoE(
  data = res,
  colorblind_friendly = T,
  x = "b",
  y = "pval",
  cutoff = 50,
  x_breaks = seq(-1, 2, 0.5),
  y_limits = c(1e-10, 1),
  vertical_lines = c(0.1, 0.9),
  hline_value = 0.1,
  title_template = "Vibration of Effects: {k} analyses with cutoff = {cutoff}"
)
```

# 2. Test cases

# 3. Usage Examples


