---
title: "Getting Started with metaMultiverse"
author: "Constantin Yves Plessen"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    fig_width: 8
    fig_height: 6
vignette: >
  %\VignetteIndexEntry{Getting Started with metaMultiverse}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6,
  fig.retina = 2,
  warning = FALSE,
  message = FALSE
)
```

# Introduction

Meta-analysis requires making many analytical decisions: Which studies to include? How to handle dependencies? Which statistical model? The **multiverse approach** systematically explores how these choices affect your conclusions.

This vignette shows you how to conduct a multiverse meta-analysis in 5 simple steps.

## Installation

```{r install, eval=FALSE}
# From GitHub
# devtools::install_github("yourname/metaMultiverse")

# Load package
library(metaMultiverse)
library(dplyr)  # For pipe operations
```

```{r load, echo=FALSE}
library(metaMultiverse)
library(dplyr)
```

## The Basic Pipeline

Every multiverse meta-analysis follows this pattern:

```{r pipeline-overview, eval=FALSE}
results <- your_data %>%
  check_data_multiverse() %>%           # Step 1: Validate data
  define_factors(...) %>%                # Step 2: Define analytical choices
  create_multiverse_specifications() %>% # Step 3: Generate all combinations
  run_multiverse_analysis()              # Step 4: Run all analyses
```

Let's walk through each step with a real example.

---

# Example: Digital Depression Interventions

We'll analyze the effectiveness of digital interventions for depression using the built-in `data_digDep` dataset.

## Load and Inspect Data

```{r load-data}
# Load example data
data("data_digDep")

# Quick look
cat("Dataset dimensions:", nrow(data_digDep), "effect sizes from",
    length(unique(data_digDep$study)), "studies\n")

# View first few rows
head(data_digDep[, c("study", "es_id", "yi", "vi", "wf_1", "wf_2", "wf_3")])
```

The dataset contains:
- `study`: Study identifier
- `es_id`: Effect size ID (unique for each effect)
- `yi`: Effect size (Hedges' g)
- `vi`: Variance of effect size
- `wf_1` through `wf_8`: "Which factors" (analytical choice variables)

---

# Step 1: Validate Your Data

The first step ensures your data is properly formatted:

```{r validate-data}
data_validated <- data_digDep %>%
  check_data_multiverse()
```

**What this checks:**
- Required columns exist (`study`, `es_id`, `yi`, `vi`)
- Data types are correct
- No non-finite values (Inf, NaN)
- Positive variances
- Unique effect size IDs
- Reasonable effect size ranges

The function returns your data (invisibly) with validation attributes, so you can pipe it directly to the next step.

---

# Step 2: Define Your Analytical Choices

This is where you specify the "forking paths" in your analysis. There are three types of decisions:

- **E (Equivalent)**: Options are theoretically interchangeable
- **U (Uncertain)**: Unclear which option is best
- **N (Non-equivalent)**: Different research questions

## Simple Example: One Factor

```{r define-simple}
factor_setup_simple <- data_validated %>%
  define_factors(
    Population = "wf_3|E"  # Population type (Equivalent decision)
  )

# View the setup
print(factor_setup_simple$factors)
```

This creates options for:
- Each individual level (e.g., "adult")
- All levels combined ("total_wf_3")

## Realistic Example: Multiple Factors

```{r define-multiple}
factor_setup <- data_validated %>%
  define_factors(
    Population = "wf_3|E",      # Adults, mixed, etc.
    Guidance = "wf_2|U",         # Guided vs. self-help (uncertain which is best)
    TimePoint = "wf_8|N"         # Post vs. follow-up (different questions!)
  )

# Summary
cat("\nFactor summary:\n")
print(factor_setup$factors)

cat("\nDecision types:\n")
print(table(factor_setup$factors$type))
```

**Important:** Type N factors create **separate multiverses**. Here, post-treatment and follow-up are analyzed separately because they answer different research questions.

## Advanced: Custom Groupings

Instead of analyzing each level individually, you can create custom groupings:

```{r define-custom}
factor_setup_custom <- data_validated %>%
  define_factors(
    # Simple factor
    Population = "wf_3|E",

    # Custom grouped factor
    Guidance = "wf_2|U|guided={guided}|minimal={unguided,minimal_guidance,no_guidance}"
  )

cat("\nCustom groupings:\n")
print(factor_setup_custom$factor_groups)
```

**Syntax:** `"column|Type|group_name={level1,level2}"`

This is useful for:
- Combining rare categories
- Testing theoretically motivated groupings
- Sensitivity analyses with different thresholds

---

# Step 3: Create All Specifications

Now generate every combination of your choices:

```{r create-specs}
specs <- factor_setup %>%
  create_multiverse_specifications(
    ma_methods = c("fe", "reml", "pm"),               # 3 methods
    dependencies = c("aggregate", "select_max")        # 2 dependency strategies
  )

cat("Specifications created:\n")
cat("  Total:", specs$number_specs, "\n")
cat("  Unique multiverses:",
    length(unique(specs$specifications$multiverse_id)), "\n")
```

**What just happened?**

The function created all valid combinations:
- 3 factors (Population, Guidance, TimePoint)
- Each E/U factor has multiple levels + "total"
- 3 meta-analytic methods
- 2 dependency handling strategies

**Type N factor effect:** Since TimePoint is Type N, we have separate multiverses for post-treatment and follow-up.

### Available Methods

```{r list-methods}
# See all available methods
cat("Available MA methods:\n")
print(list_ma_methods())
```

Methods include:
- **Standard**: FE, REML, Paule-Mandel, Hartung-Knapp
- **Bias correction**: PET-PEESE, p-uniform*, UWLS, WAAP
- **Dependency modeling**: Three-level, RVE
- **Bayesian**: bayesmeta (optional, slow)

### Dependency Strategies

Three ways to handle multiple effect sizes per study:

1. **aggregate**: Combine effects within studies (default)
2. **select_max**: Pick largest effect per study
3. **select_min**: Pick smallest effect per study
4. **modeled**: Model dependencies explicitly (3-level, RVE only)

---

# Step 4: Run All Analyses

Execute every specification:

```{r run-analyses, cache=TRUE}
results <- specs %>%
  run_multiverse_analysis(
    verbose = FALSE,  # Set TRUE to see progress details
    progress = TRUE   # Show progress bar
  )
```

**What happens:**
- Each specification is analyzed
- Failed specifications are logged (but don't stop the process)
- Results are combined into a clean data frame

### Examine Results

```{r examine-results}
cat("Analysis summary:\n")
cat("  Attempted:", results$n_attempted, "\n")
cat("  Successful:", results$n_successful, "\n")
cat("  Success rate:",
    round(100 * results$n_successful / results$n_attempted, 1), "%\n")

# Effect size summary
cat("\nEffect sizes:\n")
summary(results$results$b)

# Sample of results
cat("\nFirst few results:\n")
head(results$results[, c("b", "ci.lb", "ci.ub", "pval", "k",
                        "ma_method", "dependency")])
```

---

# Step 5: Visualize Results

The package provides two main visualizations:

## Specification Curve

Shows all effect sizes sorted by magnitude, with specification details below:

```{r spec-curve, fig.width=10, fig.height=7}
plot_spec_curve(results, interactive = FALSE)
```

**How to read this:**
- **Top panel**: Effect sizes sorted from smallest to largest
- **Bottom panels**: Which specification choices produced each result
- **Colors**: Number of studies (k) in each analysis

## Vibration of Effects (VoE)

Explores the relationship between effect size and p-value:

```{r voe-plot, fig.width=8, fig.height=6}
plot_voe(results, interactive = FALSE)
```

**How to read this:**
- **X-axis**: Effect size
- **Y-axis**: P-value (log scale)
- **Density**: Color indicates concentration of results
- **Red line**: p = 0.05 threshold

---

# Interpreting Your Multiverse

## Key Questions

### 1. Are results consistent?

```{r consistency}
# Proportion of positive effects
cat("Positive effects:",
    round(100 * mean(results$results$b > 0, na.rm = TRUE), 1), "%\n")

# Proportion significant
cat("Significant (p < .05):",
    round(100 * mean(results$results$pval < 0.05, na.rm = TRUE), 1), "%\n")

# Effect size range
cat("Effect size range: [",
    round(min(results$results$b, na.rm = TRUE), 3), ", ",
    round(max(results$results$b, na.rm = TRUE), 3), "]\n", sep = "")
```

### 2. Which choices matter most?

```{r variability}
# Effect sizes by method
results$results %>%
  group_by(ma_method) %>%
  summarise(
    n = n(),
    mean_b = mean(b, na.rm = TRUE),
    sd_b = sd(b, na.rm = TRUE)
  ) %>%
  arrange(desc(mean_b))
```

```{r variability-dependency}
# Effect sizes by dependency strategy
results$results %>%
  group_by(dependency) %>%
  summarise(
    n = n(),
    mean_b = mean(b, na.rm = TRUE),
    sd_b = sd(b, na.rm = TRUE)
  ) %>%
  arrange(desc(mean_b))
```

### 3. How do multiverses differ?

```{r multiverse-comparison}
# Compare separate multiverses (from Type N factors)
results$results %>%
  group_by(multiverse_id) %>%
  summarise(
    n_specs = n(),
    mean_b = mean(b, na.rm = TRUE),
    median_b = median(b, na.rm = TRUE),
    min_b = min(b, na.rm = TRUE),
    max_b = max(b, na.rm = TRUE)
  )
```

---

# Advanced Topics

## Setting Minimum Studies

Control the minimum number of studies required:

```{r min-studies, eval=FALSE}
# Set globally (default is 5)
options(metaMultiverse.k_smallest_ma = 10)

# Then run analysis
results <- specs %>%
  run_multiverse_analysis()
```

## Interactive Plots

For exploration, use interactive versions:

```{r interactive-plots, eval=FALSE}
# Interactive specification curve
plot_spec_curve(results, interactive = TRUE)

# Interactive VoE plot
plot_voe(results, interactive = TRUE)
```

These allow you to:
- Hover for details
- Zoom and pan
- Filter results interactively

## Filtering Results

Focus on specific subsets:

```{r filtering, eval=FALSE}
# Only REML results
reml_only <- results$results %>%
  filter(ma_method == "reml")

# Only well-powered analyses (k >= 20)
well_powered <- results$results %>%
  filter(k >= 20)

# Only one multiverse
post_only <- results$results %>%
  filter(multiverse_id == "post")
```

---

# Principled Multiverse Analysis

The E/U/N framework helps distinguish:

- **Principled multiverse** (E + U): Variations within your research question
- **Full multiverse** (E + U + N): Including different research questions

**Example interpretation:**

```{r principled-interpretation}
# If you only want to report E+U variations:
cat("Type N factors create separate analyses:\n")
cat("  Post-treatment:",
    sum(results$results$multiverse_id == "post"), "specifications\n")
cat("  Follow-up:",
    sum(results$results$multiverse_id == "fu"), "specifications\n")
cat("\nYou might report these separately in your paper,\n")
cat("rather than combining them into one analysis.\n")
```

---

# Reporting Your Multiverse

## Essential Elements

1. **Justify decisions**: Why each choice was included
2. **Specify types**: Label each as E, U, or N
3. **Show results**: Specification curve + summary statistics
4. **Interpret**: Which specifications drive variation?

## Example Summary

> "We conducted a multiverse meta-analysis with 3 analytical factors:
> Population (E), Guidance level (U), and Time point (N). Across
> `r results$n_successful` specifications, effect sizes ranged from
> g = `r round(min(results$results$b, na.rm=TRUE), 2)` to
> g = `r round(max(results$results$b, na.rm=TRUE), 2)`
> (median = `r round(median(results$results$b, na.rm=TRUE), 2)`).
> Results were significant (p < .05) in
> `r round(100 * mean(results$results$pval < 0.05, na.rm=TRUE), 0)`%
> of specifications, suggesting robust evidence for effectiveness."

---

# Common Issues

## Not enough studies

If many specifications fail:

```{r troubleshoot-k, eval=FALSE}
# Lower the threshold
options(metaMultiverse.k_smallest_ma = 3)

# Or check which specs are failing
results$multiverse_warnings
```

## Methods not compatible

Some methods only work with certain dependencies:

```{r method-compatibility, echo=FALSE}
cat("Method compatibility:\n")
cat("  Standard methods (fe, reml, pm): aggregate, select_max, select_min\n")
cat("  Three-level/RVE: modeled only\n")
cat("  p-uniform*: select_max or select_min only (not aggregate)\n")
```

## Too many specifications

Reduce by being more selective:

```{r reduce-specs, eval=FALSE}
# Fewer methods
create_multiverse_specifications(
  setup,
  ma_methods = c("reml"),  # Just one method
  dependencies = "aggregate"  # Just one strategy
)
```

---

# Next Steps

## Learn More

- See `?define_factors` for advanced factor definition
- See `?create_multiverse_specifications` for all options
- See `?plot_spec_curve` for customization

## Extend the Package

Add your own meta-analytic method (advanced):

```{r custom-method, eval=FALSE}
# Define your method
my_method <- function(data) {
  # Your meta-analysis code here
  # Must return: list(b, ci.lb, ci.ub, pval)
}

# Register it (this functionality needs to be documented)
# metaMultiverse:::register_ma_method("my_method", my_method)
```

---

# Conclusion

Multiverse meta-analysis transforms analytical flexibility from a source of concern into a tool for understanding robustness. The `metaMultiverse` package makes this approach accessible through a clean, pipeable workflow.

**Key takeaways:**

1. ✅ Use `check_data_multiverse()` to validate data
2. ✅ Use `define_factors()` with E/U/N types to specify choices
3. ✅ Use `create_multiverse_specifications()` to generate combinations
4. ✅ Use `run_multiverse_analysis()` to execute all analyses
5. ✅ Use `plot_spec_curve()` and `plot_voe()` to visualize results

---

# References

Voracek, M., Kossmeier, M., & Tran, U. S. (2019). Which data to meta-analyze, and how? A specification-curve and multiverse-analysis approach to meta-analysis. *Zeitschrift für Psychologie*, 227(1), 64-82.

Steegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. *Perspectives on Psychological Science*, 11(5), 702-712.

Del Giudice, M., & Gangestad, S. W. (2021). A traveler's guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. *Advances in Methods and Practices in Psychological Science*, 4(1), 1-15.

---

# Session Info

```{r session-info}
sessionInfo()
```
