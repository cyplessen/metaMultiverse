---
title: "Multiverse Meta-Analysis with metaMultiverse"
author: "Constantin Yves Plessen"
date: "`r Sys.Date()`"
execute:
  cache: true
  warning: false
  message: false
format:
  html:
    minimal: true
    theme: none
    css: modern-styles.css
    toc: true
    toc-location: left
    code-fold: true
    code-tools: true
vignette: >
  %\VignetteIndexEntry{Multiverse Meta-Analysis with metaMultiverse}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6,
  fig.retina = 2,
  out.width = "100%"
)
```

# Introduction 

Meta-analyses, like primary studies, involve numerous analytical decisions that can substantially impact conclusions. The metaMultiverse package extends multiverse analysis principles to meta-analysis, helping researchers navigate these "researcher degrees of freedom" while avoiding the creation of analytical "black holes" where true effects are obscured by unjustified analytical choices.

> Core Principle: A multiverse analysis should explore all reasonable analytical paths, not all possible paths. The key challenge is distinguishing between arbitrary choices (where alternatives are equally justified) and principled decisions (where some choices are objectively better).

## Theoretical Foundation
Building on Del Giudice & Gangestad's (2021) framework, we distinguish three types of nonequivalence among analytical alternatives:

1. *Measurement nonequivalence*: Different measures or indicators vary in validity and reliability
2. *Effect nonequivalence*: Different specifications estimate fundamentally different effects
3. *Power/precision nonequivalence*: Alternatives yield predictably different statistical precision

These lead to three decision scenarios that guide multiverse construction:

- Type E (Equivalence): Alternatives are effectively equivalent â†’ include in multiverse
- Type N (Nonequivalence): Some alternatives are objectively better â†’ exclude inferior options
- Type U (Uncertainty): Unclear which alternative is best â†’ explore separately

## Why This Matters
As Voracek et al. (2019) demonstrate, even multiple meta-analyses can fail to resolve contentious issues when they make different analytical choices. The metaMultiverse package provides a systematic framework to:

1. Make analytical decisions transparent and reproducible
2. Assess the robustness of conclusions across reasonable specifications
3. Avoid both overly narrow analyses (single specification) and overly broad ones (all possible specifications)

## Installation

You can install the development version of `metaMultiverse` from GitHub:

```{r installation, eval=FALSE}
# install.packages("devtools")
#devtools::install_github("cyplessen/metaMultiverse")
library(metaMultiverse)
```

## Core Workflow
The metaMultiverse workflow consists of four main steps:

### Step 1: Data Preparation and Validation

First, prepare your meta-analytic dataset with required columns:

- `study`: Study identifier (e.g., "Author, Year")
- `es_id`: Unique effect size ID
- `yi`: Effect size (following metafor conventions)
- `vi`: Sampling variance
- Factor columns: Variables representing analytical choices

This tutorial walks through each step with a practical example using digital depression intervention data.

```{r load-data}
# Load required packages
library(metaMultiverse)
library(dplyr)
library(ggplot2)
library(kableExtra)

# Load example dataset
data("data_digDep") 

# Helper function to create user-friendly column names
create_user_friendly_digDep <- function(data_digDep) {
  # Create a copy and rename the wf_* columns to meaningful names
  user_data <- data_digDep
  
  # Rename wf_* columns to what researchers would actually call them
  names(user_data)[names(user_data) == "wf_1"] <- "technology_type"      # website, mobile, etc.
  names(user_data)[names(user_data) == "wf_2"] <- "guidance_level"       # guided, minimal support, etc.
  names(user_data)[names(user_data) == "wf_3"] <- "population"           # adult, etc.
  names(user_data)[names(user_data) == "wf_4"] <- "therapy_approach"     # cbt-based, not-cbt-based, etc.
  names(user_data)[names(user_data) == "wf_5"] <- "control_type"         # waitlist, other control
  names(user_data)[names(user_data) == "wf_6"] <- "diagnostic_status"    # clinical cutoff, etc.
  names(user_data)[names(user_data) == "wf_7"] <- "risk_of_bias"         # some concerns, high risk, etc.
  names(user_data)[names(user_data) == "wf_8"] <- "time_point"           # post-treatment, follow-up
  
  return(user_data)
}

# Create user-friendly dataset
depression_data <- create_user_friendly_digDep(data_digDep)

# Examine the structure 
depression_data %>%
  select(study, es_id, yi, vi, technology_type, guidance_level, population, 
         therapy_approach, risk_of_bias) %>%
  head(10) %>%
  kable(format = "html", caption = "Digital Depression Interventions Dataset") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  ) %>%
  scroll_box(width = "100%", height = "300px")
```


### Validate data structure
```{r}
depression_data_validated <- check_data_multiverse(depression_data)
```


> **What the validator checks**
> 
> The `check_data_multiverse()` function validates:
> - Required columns are present (`study`, `es_id`, `yi`, `vi`, and factor columns)
> - Columns have correct data types
> - Effect size IDs are unique
> - No critical missing values
> - Reasonable effect size and variance values

## Step 2: Define Factors with Principled Decision Framework
The crucial step is defining which analytical choices to explore. Following Del Giudice & Gangestad's framework:


```{r define-factors}
# Define factors with custom groups and decision types
setup <- define_factors(
  depression_data_validated,
  
  # Type U: Genuine uncertainty about best approach
  Risk_of_Bias = list("risk_of_bias", 
                      decision = "U",  # Uncertain which inclusion criteria is best
                      groups = list(
                        "low_risk_only" = "low risk",
                        "exclude_high_risk" = c("low risk", "some concerns"), 
                        "all_studies" = c("low risk", "some concerns", "high risk")
                      )
  ),
  
  # Type E: Theoretically equivalent subgroups
  Technology = list(
    Technology = "technology_type", 
    decision = "E"),  # Each level individually + all combined
  
  # Type N: Non-equivalent - analyze separately
  Therapy_Approach = list(
    Therapy = "therapy_approach",
    decision = "N"  # CBT vs non-CBT are different interventions
  )
)

# The setup object contains everything we need
names(setup)
```

>Key Insight: The decision type determines how factors are handled:
> - Type E/U â†’ Create multiverse specifications
> - Type N â†’ Create separate analyses for each level

## Step 3: Create and Run Multiverse Specifications

```{r create-specs}
# Define meta-analytical methods and dependency handling
ma_methods <- c("reml", "fe", "pet-peese")
dependencies <- c("aggregate")

# Create specifications with custom groups
custom_specs <- create_principled_multiverse_specifications(
  data = setup$data,  # Use setup$data which has the wf_* columns
  wf_vars = setup$factors$wf_internal,  # Use the internal wf_* names
  ma_methods = ma_methods,
  dependencies = dependencies,
  decision_map = setup$decision_map,
  factor_groups = setup$factor_groups   # Pass the custom groups
)

# Display summary information
cat("ðŸ“Š Specification Summary:\n")
cat("   â€¢ Total specifications:", custom_specs$number_specs, "\n")
cat("   â€¢ Unique multiverses:", length(unique(custom_specs$specifications$multiverse_id)), "\n")
cat("   â€¢ Methods:", paste(ma_methods, collapse = ", "), "\n")
cat("   â€¢ Custom factor groups:", length(setup$factor_groups), "\n")

# Look at first few specifications
custom_specs$specifications %>%
  head(10) %>% 
  kable(format = "html", caption = "First 10 Specifications") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE
  )

# Run the multiverse analysis
results <- run_multiverse_analysis(
  data = setup$data,  # Use setup$data which has the wf_* columns
  specifications = custom_specs$specifications,
  factor_groups = setup$factor_groups,  # Pass custom groups for filtering
  verbose = TRUE,
  progress = TRUE
)

# View summary of results
summary_stats <- results$results %>%
  summarize(
    n_analyses = n(),
    mean_es = mean(b, na.rm = TRUE),
    median_es = median(b, na.rm = TRUE),
    min_es = min(b, na.rm = TRUE),
    max_es = max(b, na.rm = TRUE),
    prop_significant = mean(pval < 0.05, na.rm = TRUE),
    n_multiverses = length(unique(multiverse_id))
  )

summary_stats %>%
  tidyr::pivot_longer(cols = everything(), names_to = "Statistic", values_to = "Value") %>%
  mutate(
    Value = case_when(
      Statistic == "prop_significant" ~ paste0(round(Value * 100, 1), "%"),
      Statistic %in% c("n_analyses", "n_multiverses") ~ as.character(Value),
      TRUE ~ round(Value, 3) %>% as.character()
    ),
    Statistic = case_when(
      Statistic == "n_analyses" ~ "Number of Analyses",
      Statistic == "mean_es" ~ "Mean Effect Size",
      Statistic == "median_es" ~ "Median Effect Size", 
      Statistic == "min_es" ~ "Minimum Effect Size",
      Statistic == "max_es" ~ "Maximum Effect Size",
      Statistic == "prop_significant" ~ "Proportion Significant (p < 0.05)",
      Statistic == "n_multiverses" ~ "Number of Separate Multiverses",
      TRUE ~ Statistic
    )
  ) %>%
  kable(format = "html", caption = "Summary of Multiverse Meta-Analysis Results") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(row = 0, bold = TRUE, color = "white", background = "#2c3e50")
```

## Step 4: Visualize Results

The `metaMultiverse` package provides several visualization options to explore and interpret your results.

### Specification Curve

The specification curve visualizes the distribution of effect sizes across all specifications:

```{r fig.width=10, fig.height=8}
#| label: fig-spec-curve
#| fig-cap: "Specification curve showing the distribution of effect sizes across all analytical choices with custom factor groupings."

# Create a lookup table for factor labels that maps to our user-friendly names
factor_label_lookup <- list(
  wf_1 = "Risk of Bias",
  wf_2 = "Technology Type", 
  wf_3 = "Population",
  wf_4 = "Therapy Approach",
  ma_method = "Meta-Analysis Method",
  dependency = "Dependency Handling"
)

# Create the specification curve plot
plotly_descriptive_spec_curve(
  data = results$results,
  ylim_lower = -0.5,
  ylim_upper = 1.5,
  colorblind_friendly = TRUE,
  factor_label_lookup = factor_label_lookup
)
```

The specification curve consists of two panels:

1. **Top panel**: Shows the effect size estimates with confidence intervals, ordered by effect size magnitude
2. **Bottom panel**: Shows which analytical choices were made for each specification, including custom groupings


### Effect Size Distribution by Custom Groups

Let's examine how our custom risk of bias groupings affect the results:

```{r custom-groups-analysis}
# Analyze results by custom risk of bias groups
risk_bias_summary <- results$results %>%
  filter(!is.na(wf_1)) %>%  # wf_1 corresponds to Risk_of_Bias
  group_by(wf_1) %>%
  summarise(
    n_analyses = n(),
    mean_effect = mean(b, na.rm = TRUE),
    median_effect = median(b, na.rm = TRUE),
    prop_significant = mean(pval < 0.05, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(desc(mean_effect))

risk_bias_summary %>%
  kable(format = "html", 
        caption = "Effect Sizes by Risk of Bias Grouping",
        col.names = c("Risk of Bias Group", "N Analyses", "Mean Effect", 
                      "Median Effect", "Prop. Significant")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

### Vibration of Effects Plot

The Vibration of Effects plot shows the relationship between effect sizes and p-values:

```{r fig.width=8, fig.height=6}
#| label: fig-voe
#| fig-cap: "Vibration of Effects plot showing relationship between effect sizes and p-values across custom factor groupings."

plotly_VoE(
  data = results$results,
  x = "b",
  y = "pval",
  colorblind_friendly = TRUE,
  cutoff = 5,  # Minimum number of studies
  vertical_lines = c(0.1, 0.9),  # Quantile reference lines
  hline_value = 0.05  # Significance threshold
)
```

> **Interpreting the Vibration of Effects plot**
> 
> - Points represent individual meta-analyses from your multiverse
> - Color intensity represents point density
> - Horizontal line: conventional significance threshold (p = 0.05)
> - Vertical lines: effect size distribution quantiles (10th and 90th percentiles)

### Comparing Multiverse Results

Since we used "N" (Non-equivalent) decision type for therapy approach, we have separate multiverse analyses:

```{r multiverse-comparison}
# Compare results across different therapy approaches (separate multiverses)
multiverse_comparison <- results$results %>%
  group_by(multiverse_id) %>%
  summarise(
    n_analyses = n(),
    mean_effect = mean(b, na.rm = TRUE),
    median_effect = median(b, na.rm = TRUE),
    range_effect = max(b, na.rm = TRUE) - min(b, na.rm = TRUE),
    prop_significant = mean(pval < 0.05, na.rm = TRUE),
    .groups = 'drop'
  )

multiverse_comparison %>%
  kable(format = "html", 
        caption = "Comparison Across Separate Multiverse Analyses",
        col.names = c("Multiverse", "N Analyses", "Mean Effect", 
                      "Median Effect", "Effect Range", "Prop. Significant")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

## Interpreting Enhanced Multiverse Results

When interpreting results from the enhanced multiverse meta-analysis with custom factor groupings:

### 1. **Custom Group Impact**
**Hierarchical inclusion criteria**: How do different risk of bias thresholds affect conclusions?

Our custom groupings ("low_risk_only", "exclude_high_risk", "all_studies") allow us to systematically examine how inclusion criteria affect results.

### 2. **Decision Type Effects**
**Analytical structure**: How do different decision types shape the analysis?

- **U (Uncertain)** factors create multiverse options within each analysis
- **E (Equivalent)** factors create multiverse options (similar to U)
- **N (Non-equivalent)** factors create entirely separate analyses

### 3. **Range of Robustness**
**Effect stability**: How consistent are effects across custom groupings?

A narrow range of effects across sophisticated groupings suggests robust findings, while wide ranges indicate sensitivity to inclusion criteria.

## Advanced Usage with Custom Groupings

### Ordered Factor Groupings

You can create cumulative groupings for ordered factors:

```{r ordered-example, eval=FALSE}
# Example of ordered risk assessment
setup_ordered <- define_factors(
  data,
  Study_Quality = list("quality_rating",
                       type = "ordered",
                       decision = "U",
                       levels = c("high", "medium", "low"))
)
# This creates: "up_to_high", "up_to_medium", "up_to_low" groupings
```

### Complex Custom Groupings

You can create sophisticated inclusion criteria:

```{r complex-example, eval=FALSE}
setup_complex <- define_factors(
  data,
  
  # Complex intervention groupings
  Intervention_Intensity = list("intervention_type",
                                decision = "U",
                                groups = list(
                                  "minimal" = c("self_help", "bibliotherapy"),
                                  "moderate" = c("self_help", "bibliotherapy", "guided_self_help"),
                                  "intensive" = c("guided_self_help", "therapist_delivered"),
                                  "all_types" = c("self_help", "bibliotherapy", "guided_self_help", "therapist_delivered")
                                )),
  
  # Population-based groupings
  Target_Population = list("population_type",
                           decision = "N",  # Separate analyses for different populations
                           groups = list(
                             "clinical_only" = "clinical_diagnosis",
                             "subclinical_and_clinical" = c("subclinical", "clinical_diagnosis"),
                             "prevention_focus" = c("at_risk", "general_population")
                           ))
)
```

## Best Practices for Enhanced Multiverse Analysis

### 1. **Meaningful Groupings**
- Design custom groups that reflect real research decisions
- Use hierarchical groupings (restrictive â†’ permissive) when appropriate
- Ensure group names are self-explanatory

### 2. **Appropriate Decision Types**
- Use **U** when you're uncertain which criteria are best
- Use **E** when criteria are theoretically equivalent
- Use **N** when criteria represent fundamentally different research questions

### 3. **Transparent Reporting**
- Document the rationale for each custom grouping
- Report the full range of results across groupings
- Highlight which groupings drive the most variation

### 4. **Validation**
- Test your custom groupings with sensitivity analyses
- Ensure groupings align with domain expertise
- Consider pre-registering your grouping strategy

## Conclusion

The enhanced `metaMultiverse` package provides a sophisticated framework for exploring analytical decisions in meta-analysis through:

1. **User-friendly factor definitions** that use meaningful terminology
2. **Custom factor groupings** that capture complex inclusion criteria
3. **Principled decision types** that structure the analytical space appropriately
4. **Comprehensive visualizations** that communicate results effectively

By conducting and visualizing multiple analyses simultaneously with sophisticated groupings, researchers can:

- Identify which inclusion criteria significantly impact conclusions
- Present more transparent and robust meta-analytic results  
- Better understand the sensitivity of findings to methodological decisions
- Communicate uncertainty more effectively to stakeholders

> **Best practices for enhanced multiverse meta-analysis**
> 
> - Pre-register your custom grouping strategy when possible
> - Design groupings that reflect genuine research decisions
> - Use appropriate decision types (E/U/N) for your factors
> - Report the full range of results rather than cherry-picking
> - Use visualizations to communicate the multiverse of results effectively

For more information, see the package documentation or contact the developers.

## Session Information

```{r session-info}
sessionInfo()
```
